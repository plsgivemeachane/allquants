---
license: apache-2.0
base_model: {{base_model_w_author}}
pipeline_tag: text-generation
---

# ğŸ¤– Model Card for {{base_model}}-GGUF

This repo is packed with multiple quantized versions of {{base_model_w_author}} in GGUF format. ğŸš€âœ¨  
Built for running efficiently on your everyday hardware - no need for enterprise-level specs to deploy these models. ğŸ’»ğŸ¯ğŸ”¥  

## ğŸ“‹ Model Details

### âš¡ Quantization Results

| Quantization | Size (vs. FP16) | Speed     | Quality    | Recommended For                      |
|--------------|-----------------|-----------|------------|--------------------------------------|
| Q2_K         | Tiny ğŸ­         | Lightning âš¡ | Basic ğŸ“‰   | Quick prototypes, potato hardware ğŸ§ª |
| Q3_K_S       | Mini ğŸ¹         | Super fast ğŸš€| Decent ğŸ“Š  | Mobile devices, quick tests ğŸ“±      |
| Q3_K_M       | Small ğŸ°        | Fast ğŸ’¨   | Good ğŸ“ˆ    | Lightweight but better quality      |
| Q3_K_L       | Small+ ğŸ±       | Fast âš¡   | Good ğŸ“Š    | Speed with acceptable quality       |
| Q4_0         | Medium ğŸº       | Quick âš¡   | Solid ğŸ‘   | Daily driver, casual chats ğŸ’¬       |
| Q4_1         | Medium ğŸ¦Š       | Quick ğŸš€   | Solid+ ğŸ‘Œ  | Slight upgrade from Q4_0            |
| Q4_K_S       | Medium ğŸ»       | Quick ğŸ’¨   | Nice âœ¨    | Well-balanced choice âš–ï¸            |
| Q4_K_M       | Medium ğŸ¦       | Quick âš¡   | Really nice ğŸŒŸ| The crowd favorite ğŸ…         |
| Q5_0         | Chunky ğŸ˜       | Chill ğŸš¶  | Great ğŸ’ª   | Chatbots that actually make sense ğŸ¤–|
| Q5_1         | Chunky ğŸ¦       | Chill â³  | Great+ ğŸ”¥  | When you need quality responses ğŸ’¼  |
| Q5_K_S       | Big ğŸ³          | Chill ğŸ•  | Great+ â­  | For the quality-conscious ğŸ¯        |
| Q5_K_M       | Big ğŸ¦£          | Chill âŒ›  | Excellent ğŸ†| High-end performance ğŸ’           |
| Q6_K         | Massive ğŸ‹      | Slow ğŸŒ   | Near perfect ğŸ‘‘| Enthusiasts only              |
| Q8_0         | Absolute unit ğŸ¦•| Turtle ğŸ¢ | Basically perfect ğŸ’| Max settings gang ğŸ–¥ï¸    |

> **ğŸ“ Real talk:**  
> - Lower numbers = smaller files ğŸ“‰, runs faster âš¡, but quality takes a hit ğŸ“Š  
> - Q4_K_M hits different - it's the sweet spot most people actually want ğŸ‘¥  
> - Q6_K/Q8_0 are for perfectionists with beefy hardware ğŸ†ğŸ§™â€â™‚ï¸  
> - Everything here runs on regular consumer hardware ğŸ’» - pick what matches your vibe! ğŸ¯


### ğŸ“ Model Description

- **Quantized by:** leeminwaan ğŸ‘¨â€ğŸ’»  
- **Funded by [optional]:** Solo project, no corporate backing ğŸ’°  
- **Shared by [optional]:** leeminwaan ğŸ¤  
- **Model type:** Decoder-only transformer (the good stuff) ğŸ§ ğŸ¤–  
- **Language(s) (NLP):** Base on {{base_model}}
- **License:** Apache-2.0 (free to use, modify, distribute) ğŸ“„âš–ï¸  

### ğŸ”— Model Sources

- **Repository:** [Hugging Face Repo](https://huggingface.co/leeminwaan/{{base_model}}-GGUF) ğŸ¤—ğŸ“¦  
- **Quantization Tool:** [AllQuants](https://github.com/plsgivemeachane/allquants) ğŸ”¢âš¡  
- **Paper [optional]:** No research paper (this is practical, not academic) ğŸ“âŒ  
- **Demo [optional]:** Demo coming soonâ„¢ ğŸ®ğŸ”œ  

## ğŸš€ How to Get Started with the Model

```python
# ğŸ Quick start - literally just this:
from huggingface_hub import hf_hub_download

# ğŸ“¥ Grab the model (Q4_K_M is the sweet spot for most people)
model_path = hf_hub_download("leeminwaan/{{base_model}}-GGUF", "{{base_model}}-q4_k_m.gguf")
print("Downloaded:", model_path) # ğŸŠ You're good to go!
````

Available flavors: ğŸğŸ“¦

* Q2\_K, Q3\_K\_S, Q3\_K\_M, Q3\_K\_L ğŸƒâ€â™‚ï¸ğŸ’¨ (Speed demons - perfect for testing)
* Q4\_0, Q4\_1, Q4\_K\_S, Q4\_K\_M âš–ï¸âœ¨ (The goldilocks zone - just right)
* Q5\_0, Q5\_1, Q5\_K\_S, Q5\_K\_M ğŸ’ªğŸ¯ (For when you need that extra quality)
* Q6\_K, Q8\_0 ğŸ†ğŸ‘‘ (Maxed out settings - if your hardware can handle it)

## ğŸ¯ Training Details

### ğŸ“Š Training Data

* This is a straight quantization - no extra training or fine-tuning involved. âœ¨

### âš™ï¸ Training Procedure

* Took {{base_model_w_author}} and compressed it into these GGUF formats. ğŸ”„

## ğŸ”§ Technical Specifications

#### ğŸ’¾ Software

* llama.cpp for the heavy lifting ğŸ¦™
* Python 3.10 + huggingface_hub for the workflow ğŸ

## ğŸ“š Citation

**BibTeX:** ğŸ“–ğŸ”¬

```bibtex
@misc{{base_model}}-GGUF,
  title={{base_model}}-GGUF Quantized Models},
  author={leeminwaan},
  year={2025}, % ğŸŠ Hot off the press!
  howpublished={\url{https://huggingface.co/leeminwaan/{{base_model}}-GGUF}}
}
```

**APA:** ğŸ“âœ¨

```
leeminwaan. (2025). {{base_model}}-GGUF Quantized Models [Computer software]. ğŸ’» Hugging Face. https://huggingface.co/leeminwaan/{{base_model}}-GGUF ğŸ¤—
```

## ğŸ“– Glossary

* **Quantization:** Making models smaller by reducing number precision - trades some quality for efficiency. ğŸ”¢
* **GGUF:** The file format that llama.cpp loves - optimized for fast inference. âš¡

## â„¹ï¸ More Information

* This is still a work in progress - expect some rough edges. ğŸ§ª
* More updates and proper benchmarks coming when I get around to it. ğŸ“ˆ

## ğŸ‘¨â€ğŸ’» Model Card Authors

* leeminwaan ğŸš€ğŸ‘¨â€ğŸ’»âœ¨

## ğŸ“§ Model Card Contact

* Hugging Face: [leeminwaan](https://huggingface.co/leeminwaan) ğŸ¤—ğŸ’ŒğŸ‰
